---
layout: post
hidden: true
title:  "Fifty shades of research"
---

<!---
Doctoral Seminar, Collaborative Specialization in Global Health
CHL5701H Syllabus [fall 2024/winter 2025]

First Assignment
i) All students will be required to prepare an article for submission to Juxtaposition
(https://juxtamagazine.org/) or another publication venue of your choice (e.g.
DLSPH’s Centre for Global Health’s newsletter, the Conversation, etc.) in
consultation with the instructor. The objective of this assignment is to develop skills
for communicating research to non-academic audiences. Students are encouraged
to consider articles related to current events, talks, symposia, and their own
research. Please submit to the instructor by December 12th, 2024 at the latest for
feedback before submitting to the publication venue. 
--->

> What counts as bad health research? There is no single answer. This article presents three distinct cases of controversial research and introduces the reader into scholarly debate about research evaluation.

## Case 1: Landmark Paper Retracted over Images Suspicious of Manipulation

In June 2024, a scientific paper published 18 years ago was retracted. This paper is now considered one of the [most influential papers ever retracted](https://retractionwatch.com/the-retraction-watch-leaderboard/top-10-most-highly-cited-retracted-papers/).

“A specific amyloid-β protein assembly in the brain impairs memory,” read the [original title](https://doi.org/10.1038/nature04533). After [concerns were raised](https://pubpeer.com/publications/8FF7E6996524B73ACB4A9EF5C0AACF#) about the trustworthiness of the figures on which this conclusion was based, a two-year online discussion and an [investigation](https://www.science.org/content/article/potential-fabrication-research-images-threatens-key-theory-alzheimers-disease) ensued. In consequence, one of the paper’s senior authors declared their decision to [retract](https://doi.org/10.1038/s41586-024-07691-8) the paper.

This case was highly publicized by _[Science](https://www.science.org/content/article/researchers-plan-retract-landmark-alzheimers-paper-containing-doctored-images)_ and news media outlets. The [Retraction Watch](https://retractionwatch.com/the-retraction-watch-leaderboard/top-10-most-highly-cited-retracted-papers/) reports, as of December 12, 2024, that the paper itself was cited over 2,300 times, including more than 10 citations after the retraction note was published. One of the reasons for this study’s popularity is its landmark role in Alzheimer’s disease research, with its retraction [casting a shadow](https://en.wikipedia.org/wiki/Sylvain_Lesn%C3%A9#Impact_on_Alzheimer's_research) on the consequent research and translation efforts.

## Case 2: Five Hundred Excessive Deaths in Patients Not Offered an Effective Drug

“Of more than 2000 redundant clinical trials on statins in patients with coronary artery disease… an extra 3000 MACEs, including nearly 600 deaths, were experienced by participants not treated with statins in these trials.”

This is the conclusion that was reached after [investigating the aftermath](https://doi.org/10.1136/bmj.n48 ) of a clinical cardiology practice guideline introduced in 2007 in China. This guideline included a strong recommendation, based on high-quality evidence, for administering a statin medication to patients with two common heart diseases, stable angina pectoris and acute coronary syndrome. It was then logical to assume, the investigators argue, that any new trials conducted after that point (plus a one-year lag to account for the guideline adoption) were to be considered redundant. The investigators focused on randomized clinical trials where, by design, a random sample of eligible heart disease patients would receive the recommended drug (statin), and the rest of the patients would be offered a placebo pill or no treatment.

2045 redundant trials like that were identified in academic literature. In the no-active drug group, 3470 extra major adverse cardiac events (MACEs) were reported, including 559 deaths, 973 patients with new or recurrent myocardial infarction, 161 patients with stroke, 83 patients requiring revascularization, 398 patients with heart failure, 1197 patients with recurrent or deteriorated angina pectoris, and 99 unspecified MACEs. The science behind randomized trials dictates that should these people be offered a statin, then these outcomes could have been prevented.

The results of this investigation were published in the reputable _BMJ_ journal back in 2021. Almost four years after its publication, it has not been widely publicized. As of December 12, 2024, the publication’s [Altmetric profile](https://www.altmetric.com/details/99262997) shows that it was only ever picked up by two news outlets, one blog, and 55 social media posts. It has also received only 15 to 16 academic citations (based on [Google Scholar](https://scholar.google.com/scholar?cites=13369986014045436949) or [OpenAlex](https://openalex.org/works?filter=cites%3Aw3129053163) statistics, respectively).

## Case 3: The False Promise of Meta-research

Meta-research “is about ensuring that we have the evidence we need to realise the full potential of research” – [Research on Research Institute](https://researchonresearch.org/about/). One of the methods of meta-research is evidence synthesis, that is, “the way that academics bring together knowledge from across multiple studies into a whole, to present the state of current under-standing about a given area.” ([Handbook of Meta-Research](https://doi.org/10.4337/9781839105722)).

Going back to the study about statin trials, let us see how it is reflected in the meta-research about this topic. Naturally, most of the 15 or 16 academic articles that cited it can be categorized as meta-research studies, and as evidence syntheses. However, a recently published [review of the methods](https://doi.org/10.7717/peerj.18466) for assessing such cases is not among them. Under “such cases” I refer, of course, to research that is known to be controversial but is not, anyhow, broadly condemned as “alleged misconduct” or a “questionable research practice.” These gray zone cases are sometimes referred to as [research waste](https://doi.org/10.1111/jebm.12616).

Upon closer examination, the reason why the statin trials investigation was missed was because the review’s search strategy only included the word _redundancy_ as part of a phrase “redundant research;” whereas, the statin trials paper used slightly different wording, namely “redundant clinical trials.” The consultation and approval of the search strategy by an information specialist has failed to safeguard the study against this unforeseen variation, as did the strict keyword-based nature of top-tier academic database search systems such as PubMed. Who would have the courage to immediately categorize this evidence synthesis (excellent in many other aspects!) as research waste?

Overall, things look quite dismal in evidence synthesis. For example, a recent finding was that [78% of systematic reviews do not have a reproducible search strategy](https://doi.org/10.1016/j.jclinepi.2023.111229), considered to be a key feature of this kind of a meta-research study. Another estimate contends that [97% of systematic reviews either do not have adequate methods or are clinically useless](https://doi.org/10.1111/1468-0009.12210). In broader meta-research, there is [not enough unanimity](https://doi.org/10.1097/XEB.0000000000000201) even on the preferred terminology to denote the field, let alone funding and governance issues. Apparently, the field does not fully deliver its promise.

<!---probably won't talk about this because this is going too niche for a wide audience
- (?) maybe to throw in some words about 'the false promise of meta-research', eg the covid nma lnma case or scandal or irreproducibility of sys revs - but this may be too niche for an article for broad audience--->
<!---lacking evidence to support these claims
- (?) elements of traditionalism and witch hunting to what is condemned more, historically what was easier and more socially favorable to condemn, may be more often condemned - 
--->
<!---pretty obvious
- so this is all about research cultures
--->
<!---pretty niche too but will maybe include somewhere
- in the context of my own phd thesis research where we want to build a tool to measure research waste, this begins to emerge as the more and more wicked to me
--->

## 

more thoughts after reading kolstoe & pugh thoroughly, skimming through some works that cited it, and some works referenced therein, and then after also skimming through abramo and coara

Simon E. Kolstoe & Jonathan Pugh (2023). "The trinity of good research: Distinguishing between research integrity, ethics, and governance" https://doi.org/10.1080/08989621.2023.2239712

- kolstoe & pugh make a commendable attempt to set normative boundaries between integrity, ethics, and governance, although they themselves acknowledge that there also are other reshapings of these norms (eg printeger report 2016 or allea's ecoc for ri 2023 that do not seem to differentiate between ethics and integrity)

PRINTEGER (Promoting Integrity as an Integral Dimension of Excellence in Research), Chapter "Normative analysis of research integrity and misconduct" (2016) https://printeger.eu/wp-content/uploads/2016/10/D2.3.pdf

ALLEA’s The European Code of Conduct for Research Integrity (revised 2023, includes generative AI provisions) https://allea.org/code-of-conduct/

- and even despite that they mention the interconnectedness between the trinity, it is questionable id it is indeed in such a weak form like the puzzle elements that they suggest; arguably there is a lot tighter interconnections, so tight that this might actually be better conceptualized by saying that both "pure" ethics and integrity represent variants of expert assessment (with an ethics professional and research professional as assessors, respectively) whereas the governance domain remains, representing the institutionalization of these assessment
- it should also be mentioned that these two aspects would not be separate but are maybe best conceptualized as a dialectic unity and conflict, best exemplified with goodhart's law

John Michael Roberts (2014). "Critical realism, dialectics, and qualitative research methods" https://doi.org/10.1111/jtsb.12056

https://en.wikipedia.org/wiki/Goodhart%27s_law

- that streamlined approach is stronger in that it provides theoretical coverage for a case that both kolstoe & pugh and some other norm setters apparently fail to account for, namely weaker cases of research integrity which do not fall under misconduct principles and policies but are still criticized from the particular research culture standpoint, and sometimes there are even policies in place (eg reporting checklists); it is interesting that these are even not covered by allea's ecoc for ri that brings a broad lens to integrity that also includes reproducibility, for instance, but in the evaluative aspect only talks about misconduct and qrp
- it is illustrative that this discussion happens amidst a continent wide and potentially global scale conflict around coara, which ultimately comes down to quant vs qual assessment/eval of research, and importantly researcher/research institutions

CoARA (2022) https://coara.eu/

Pushback from Giovanni Abramo, President of the International Society for Scientometrics and Informetrics, ISSI, published 11 May 2024: "The forced battle between peer-review and scientometric research assessment: Why the CoARA initiative is unsound" https://doi.org/10.1093/reseval/rvae021

<!---probably won't talk about this because this is too theoretical for a wide audience
- so generalizing on all that, there are apparently two axes here when talking about shades of research. one axis, let it be X axis to mimic the domain of the "research shades function", is the moral to evaluative intent, so it's whether we're just saying something is right or wrong or want to point fingers, or someplace in between. The Y axis, or this sort of codomain of this intent is the actual implementation which is a spectrum from fully pep talk starus or informal to fully institutionalized. so i argue that all kinds of axiologic judgments about research, or researchERs or research systems for that matter, that is, any attempts to add a measurement dimension to research, can be mapped someplace on that XY coordinate map.
- it's funny that all that only makes sense when we think we know what research is vs non research, eg the demarcation problem. if we also wanted to account for that we would need to add a third, Z axis here that would "measure" how research-y or unresearch-y this or that research-related thing really is (and i'm not saying only scientific/unscientific because i assume that there is also some discussion space around research being broader than science and also encompassing eg r&d within for profit companies, or mythbusters kind of entertainment, or whatever)
--->
