<!---
Doctoral Seminar, Collaborative Specialization in Global Health
CHL5701H Syllabus [fall 2024/winter 2025]

First Assignment
i) All students will be required to prepare an article for submission to Juxtaposition
(https://juxtamagazine.org/) or another publication venue of your choice (e.g.
DLSPH’s Centre for Global Health’s newsletter, the Conversation, etc.) in
consultation with the instructor. The objective of this assignment is to develop skills
for communicating research to non-academic audiences. Students are encouraged
to consider articles related to current events, talks, symposia, and their own
research. Please submit to the instructor by December 12th, 2024 at the latest for
feedback before submitting to the publication venue. 
--->

> A version of this article was originally published in the University of Toronto’s [Centre for Global Health Bulletin, March 2025 Newsletter](https://mailchi.mp/913d9cc555c0/centre-for-global-health-march-2025-newsletter). All changes are tracked in the [version history on GitHub](https://github.com/drzhelnov/aware/commits/main/_posts/2024-12-12-fifty-shades-of-research.md).

## Summary

What counts as _bad_ health research? There is no single answer. This article presents (not fifty but) three distinct cases of controversial biomedical, clinical, and “meta” research and introduces a broad audience to the topic of research waste. Research waste refers to the inefficient, ineffective, or unintentionally harmful use of resources (e.g., financial, human, time, participant burden, or excess use of animals) at any level of the health research ecosystem.

## Case 1: Landmark paper retracted over images suspicious of manipulation

In June 2024, a scientific paper published 18 years ago was retracted [@lesne_specific_2006]. This paper is now considered one of the [most influential papers ever retracted](https://retractionwatch.com/the-retraction-watch-leaderboard/top-10-most-highly-cited-retracted-papers/). After [concerns were raised online](https://pubpeer.com/publications/8FF7E6996524B73ACB4A9EF5C0AACF#) about the trustworthiness of the figures on which its conclusion was based, an online discussion and an [investigation](https://www.science.org/content/article/potential-fabrication-research-images-threatens-key-theory-alzheimers-disease) by the journal ensued. In consequence, the authors [retracted](https://doi.org/10.1038/s41586-024-07691-8) the paper.

This case was highly publicized by _[Science](https://www.science.org/content/article/researchers-plan-retract-landmark-alzheimers-paper-containing-doctored-images)_ and other media. The paper itself was cited [over 2,300 times](https://retractionwatch.com/the-retraction-watch-leaderboard/top-10-most-highly-cited-retracted-papers/), including more than 10 citations after the retraction note was published. One of the reasons for this study’s popularity is its landmark role in Alzheimer’s disease research: after years of failure to develop an effective Alzheimer’s medication, this study provided experimental evidence of a [promising drug target](https://doi.org/10.1038/440284a). Its retraction [cast a slight shadow](https://en.wikipedia.org/wiki/Sylvain_Lesn%C3%A9#Impact_on_Alzheimer's_research) on the consequent research and translation efforts.

## Case 2: Five hundred excessive deaths in patients not offered an effective drug

“Of more than 2000 redundant clinical trials on statins in patients with coronary artery disease… an extra 3000 [major adverse cardiac events], including nearly 600 deaths, were experienced by participants not treated with statins in these trials.”

This is the conclusion reached after [investigating the aftermath](https://doi.org/10.1136/bmj.n48) of a clinical cardiology practice guideline introduced in 2007 in China. This guideline included a strong recommendation, based on high-quality evidence, for administering a statin medication to patients with two common heart diseases: stable angina pectoris or acute coronary syndrome. It was then logical to assume, the investigators argued, that any new trials conducted after that point (plus a one-year lag to account for the guideline adoption) were to be considered redundant, and 3470 people harmed.

The results of this investigation were published in the reputable _British Medical Journal (BMJ)_ journal back in 2021 [@jia_effect_2021]. Almost four years after its publication, it has not been widely publicized. The publication’s [Altmetric profile](https://www.altmetric.com/details/99262997) shows that it was only ever picked up by two news outlets, one blog, and 53 social media posts. It has also received only [18 academic citations](https://openalex.org/works?filter=cites%3Aw3129053163).

## Case 3: The false promise of meta-research

A recently published [review of methods](https://doi.org/10.7717/peerj.18466) for assessing research “waste” is not among the 18 papers that cited the statin trials study. Research waste includes practices that are known to be controversial but are not broadly condemned as [questionable (unacceptable)](https://allea.org/code-of-conduct/) or as alleged misconduct [@rosengaard_five_2024]. Upon examination, the reason why the statin trials study was missed was because of a slight difference in the wording used in the study abstract and the review’s search strategy. This is despite the rigorous design and state-of-the-art methods employed by the review authors [@rosengaard_several_2024].

Evidence syntheses of health research are designed to be “the way that academics bring together knowledge from across multiple studies into a whole, to present the state of current understanding about a given area” [@thomas_methods_2024]. There are multiple examples of how the field does not fully deliver its promise. A recent finding was that [78% of systematic reviews do not have a reproducible search strategy](https://doi.org/10.1016/j.jclinepi.2023.111229), considered to be a key feature of this kind of meta-research study [@rethlefsen_systematic_2024]. Another estimate contends that [97% of systematic reviews either do not have adequate methods or are clinically useless](https://doi.org/10.1111/1468-0009.12210) [@ioannidis_mass_2016]. There is [not enough unanimity](https://doi.org/10.1097/XEB.0000000000000201) even on the preferred terminology to denote meta-research itself, let alone on how to collaborate efficiently [@puljak_methodological_2019].

<!---probably won't talk about this because this is going too niche for a wide audience
- (?) maybe to throw in some words about 'the false promise of meta-research', eg the covid nma lnma case or scandal or irreproducibility of sys revs - but this may be too niche for an article for broad audience--->
<!---lacking evidence to support these claims
- (?) elements of traditionalism and witch hunting to what is condemned more, historically what was easier and more socially favorable to condemn, may be more often condemned - 
--->
<!---pretty obvious
- so this is all about research cultures
--->
<!---pretty niche too but will maybe include somewhere
- in the context of my own phd thesis research where we want to build a tool to measure research waste, this begins to emerge as the more and more wicked to me
--->

## AWARE of research waste

This article presented three distinct cases of health research that can, ostensibly, be called “bad.” The cases range from the darkest shades (Case 1: a retracted article due to alleged misconduct) through a well-declared gray zone (Case 2: research explicitly called redundant in a meta-research study) or even an area genuinely challenging to categorize on a black and white spectrum (Case 3: integral challenges within meta-research itself).

The difference, however, seems to be primarily in the degree of community consensus toward the label (misconduct vs. waste vs. “challenges”), not necessarily the impacts of the discussed research. For example, the largely unnoticed Case 2 provides evidence for well-documented, and drastic, negative health outcomes whereas, for the widely condemned and retracted Case 1, the direct health impacts are more elusive.

A broader scholarly debate questions whether a research work should be evaluated based on its “quality” in and of itself or based on its “impacts” on the world. A notable discussion happened around a 2022 initiative called the [Coalition for Advancing Research Assessment (CoARA)](https://coara.eu/). The coalition calls for wider adoption of peer review in research assessment, in opposition to publication-based metrics. This initiative received [strong pushback](https://doi.org/10.1093/reseval/rvae021) from the president of the International Society for Scientometrics and Infometrics (ISSI) [@abramo_forced_2024].

It is clear that no unanimous position exists in the academic community as to what exactly constitutes bad health research. Complete clarity about what we mean by research waste and how we measure it is urgently needed to develop evidence-based strategies for easing its insidious pressure on health care decision-makers, providers, and patients globally.

Our project, called [Avoidable WAste in health REsearch (AWARE)](https://researchwaste.info/about/), aims to look more into this.

## Acknowledgments

I would like to thank Erica Di Ruggiero and Michelle Christian for their helpful feedback on an earlier version of this article, the Scientometrics Centre at the HSE University for their informative news feed, my supervisor Andrea Tricco for the opportunities to study this topic, and the many other colleagues who shared their insights about research waste over the past 2 years.

<!---probably won't talk about this because this is too theoretical for a wide audience
- kolstoe & pugh make a commendable attempt to set normative boundaries between integrity, ethics, and governance, although they themselves acknowledge that there also are other reshapings of these norms (eg printeger report 2016 or allea's ecoc for ri 2023 that do not seem to differentiate between ethics and integrity)

PRINTEGER (Promoting Integrity as an Integral Dimension of Excellence in Research), Chapter "Normative analysis of research integrity and misconduct" (2016) https://printeger.eu/wp-content/uploads/2016/10/D2.3.pdf

ALLEA’s The European Code of Conduct for Research Integrity (revised 2023, includes generative AI provisions) https://allea.org/code-of-conduct/

- and even despite that they mention the interconnectedness between the trinity, it is questionable id it is indeed in such a weak form like the puzzle elements that they suggest; arguably there is a lot tighter interconnections, so tight that this might actually be better conceptualized by saying that both "pure" ethics and integrity represent variants of expert assessment (with an ethics professional and research professional as assessors, respectively) whereas the governance domain remains, representing the institutionalization of these assessment
- it should also be mentioned that these two aspects would not be separate but are maybe best conceptualized as a dialectic unity and conflict, best exemplified with goodhart's law

John Michael Roberts (2014). "Critical realism, dialectics, and qualitative research methods" https://doi.org/10.1111/jtsb.12056

https://en.wikipedia.org/wiki/Goodhart%27s_law

- that streamlined approach is stronger in that it provides theoretical coverage for a case that both kolstoe & pugh and some other norm setters apparently fail to account for, namely weaker cases of research integrity which do not fall under misconduct principles and policies but are still criticized from the particular research culture standpoint, and sometimes there are even policies in place (eg reporting checklists); it is interesting that these are even not covered by allea's ecoc for ri that brings a broad lens to integrity that also includes reproducibility, for instance, but in the evaluative aspect only talks about misconduct and qrp
- so generalizing on all that, there are apparently two axes here when talking about shades of research. one axis, let it be X axis to mimic the domain of the "research shades function", is the moral to evaluative intent, so it's whether we're just saying something is right or wrong or want to point fingers, or someplace in between. The Y axis, or this sort of codomain of this intent is the actual implementation which is a spectrum from fully pep talk starus or informal to fully institutionalized. so i argue that all kinds of axiologic judgments about research, or researchERs or research systems for that matter, that is, any attempts to add a measurement dimension to research, can be mapped someplace on that XY coordinate map.
- it's funny that all that only makes sense when we think we know what research is vs non research, eg the demarcation problem. if we also wanted to account for that we would need to add a third, Z axis here that would "measure" how research-y or unresearch-y this or that research-related thing really is (and i'm not saying only scientific/unscientific because i assume that there is also some discussion space around research being broader than science and also encompassing eg r&d within for profit companies, or mythbusters kind of entertainment, or whatever)
--->
